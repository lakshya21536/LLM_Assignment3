{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-04T17:58:58.371790Z","iopub.execute_input":"2024-11-04T17:58:58.372186Z","iopub.status.idle":"2024-11-04T17:58:58.732916Z","shell.execute_reply.started":"2024-11-04T17:58:58.372136Z","shell.execute_reply":"2024-11-04T17:58:58.732064Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:58:58.734671Z","iopub.execute_input":"2024-11-04T17:58:58.735171Z","iopub.status.idle":"2024-11-04T17:59:22.675309Z","shell.execute_reply.started":"2024-11-04T17:58:58.735104Z","shell.execute_reply":"2024-11-04T17:59:22.674412Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\nDownloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.0\n    Uninstalling pip-24.0:\n      Successfully uninstalled pip-24.0\nSuccessfully installed pip-24.3.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft trl bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:59:22.676667Z","iopub.execute_input":"2024-11-04T17:59:22.677006Z","iopub.status.idle":"2024-11-04T17:59:42.674298Z","shell.execute_reply.started":"2024-11-04T17:59:22.676971Z","shell.execute_reply":"2024-11-04T17:59:42.673343Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nCollecting transformers (from peft)\n  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\nDownloading trl-0.12.0-py3-none-any.whl (310 kB)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, transformers, trl, peft\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed bitsandbytes-0.44.1 peft-0.13.2 transformers-4.46.1 trl-0.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install and import the necessary libraries\n# !pip install torch peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 accelerate einops \n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    AutoTokenizer,\n    TrainingArguments,\n    pipeline,\n)\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:59:42.677215Z","iopub.execute_input":"2024-11-04T17:59:42.677943Z","iopub.status.idle":"2024-11-04T18:00:02.151911Z","shell.execute_reply.started":"2024-11-04T17:59:42.677906Z","shell.execute_reply":"2024-11-04T18:00:02.150951Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Model\nbase_model = \"microsoft/phi-2\"\nnew_model = \"phi-2-finetuned\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:00:02.153229Z","iopub.execute_input":"2024-11-04T18:00:02.153948Z","iopub.status.idle":"2024-11-04T18:00:02.157970Z","shell.execute_reply.started":"2024-11-04T18:00:02.153900Z","shell.execute_reply":"2024-11-04T18:00:02.156956Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nimport torch\n\ndef load_and_sample_snli():\n    dataset = load_dataset(\"stanfordnlp/snli\")\n    \n    # Sampling every 550th sample for training, every 100th for test and validation\n    train_samples = dataset['train'].select(range(0, len(dataset['train']), 550))[:1000]\n    test_samples = dataset['test'].select(range(0, len(dataset['test']), 100))[:100]\n    validation_samples = dataset['validation'].select(range(0, len(dataset['validation']), 100))[:100]\n    \n    return DatasetDict({\n        'train': Dataset.from_dict(train_samples),\n        'test': Dataset.from_dict(test_samples),\n        'validation': Dataset.from_dict(validation_samples)\n    })\n\ndataset = load_and_sample_snli()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:00:02.159344Z","iopub.execute_input":"2024-11-04T18:00:02.159614Z","iopub.status.idle":"2024-11-04T18:00:05.203316Z","shell.execute_reply.started":"2024-11-04T18:00:02.159585Z","shell.execute_reply":"2024-11-04T18:00:05.202530Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/16.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"641cabb2f595460693b3a10642c4cae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/412k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43516321cfb54afb9e034adfb8ea580e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/413k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91435125f0214c1d876c3a342a2ba7f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/19.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6782348a37c41e182410adeb5d252e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37fcfb192508454c842151de368071b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b13f8649d244fa681e435c426ceffb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/550152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8ac19d67fd04d178cf24a1b79d1cb95"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:00:05.204579Z","iopub.execute_input":"2024-11-04T18:00:05.204963Z","iopub.status.idle":"2024-11-04T18:00:06.513550Z","shell.execute_reply.started":"2024-11-04T18:00:05.204919Z","shell.execute_reply":"2024-11-04T18:00:06.512431Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccc5c705e1274c31bca97f04fbc92093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a5e85c80db7456b83e113acf9115736"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ecac41c64d4441c8539b44748d2c80a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c1b3f62fb064595a14438bcf56eca80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66eb74a6c6049c085215976f33ffef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4da135960704277b31c198071962174"}},"metadata":{}}]},{"cell_type":"code","source":"# Quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Load base moodel\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    device_map={\"\": 0}\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:00:06.514802Z","iopub.execute_input":"2024-11-04T18:00:06.515141Z","iopub.status.idle":"2024-11-04T18:02:23.072585Z","shell.execute_reply.started":"2024-11-04T18:00:06.515094Z","shell.execute_reply":"2024-11-04T18:02:23.071612Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1877d89aac14bcfafd7f5af67ba95db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe55a11b35f84004a7417ba7f9b2eedd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008820f94a734389b076519a452e47bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a7bcb019054bc183717e68394a48c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9920ff2a98364babaf58ce191d002f1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ace24ac89e4efda63b198480a905cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70ab327c9d941ecb8aeeedd1dc5e6c0"}},"metadata":{}}]},{"cell_type":"code","source":"def generate_prompt(premise, hypothesis):\n    return (\n        f\"Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\n\"\n        f\"Premise: {premise}\\n\"\n        f\"Hypothesis: {hypothesis}\\n\"\n        f\"Choose the best answer:\\n\"\n        f\"If Hypothesis entails the Premise, Answer: 0\\n\"\n        f\"If Neutral, Answer: 1\\n\"\n        f\"If Hypothesis does not entail the Premise, Answer: 2\\n\"\n        f\"Answer only one numerical value. \\n\\n\"\n        f\"Answer:\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:02:23.074191Z","iopub.execute_input":"2024-11-04T18:02:23.074892Z","iopub.status.idle":"2024-11-04T18:02:23.080148Z","shell.execute_reply.started":"2024-11-04T18:02:23.074847Z","shell.execute_reply":"2024-11-04T18:02:23.079467Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Define mapping of labels\nlabel_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\npretrain_outputs = []\n# Evaluate on the test set\ncorrect = 0\ntotal = 0\n\nfor example in dataset['test']:\n    # Create prompt from premise and hypothesis\n    prompt = generate_prompt(example['premise'], example['hypothesis'])\n    \n    # Generate model's response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    output = model.generate(**inputs, max_new_tokens=15)\n    response = tokenizer.decode(output[0], skip_special_tokens=True).strip().lower()\n#     print(f\"Model response: {response}\")\n    \n    # Find the last occurrence of \"answer: 0\", \"answer: 1\", or \"answer: 2\" in reverse order\n    last_answer = None\n\n    for ans in [\"answer: 0\", \"answer: 1\", \"answer: 2\"]:\n        pos = response.rfind(ans)  # Search from the end of the text\n        if pos != -1:\n            # Keep track of the last found answer if it appears later in the text\n            if last_answer is None or pos > response.rfind(last_answer):\n                last_answer = ans\n\n    output = None\n    if(last_answer == \"answer: 0\"):\n        output = 0\n    elif(last_answer == \"answer: 1\"):\n        output = 1\n    elif(last_answer == \"answer: 2\"):\n        output = 2\n    \n    if output == example['label']:\n        correct += 1\n    total += 1\n    pretrain_outputs.append(output)\n# Calculate and print accuracy\naccuracy = correct / total\nprint(f\"Accuracy: {accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:02:23.084572Z","iopub.execute_input":"2024-11-04T18:02:23.085198Z","iopub.status.idle":"2024-11-04T18:03:41.407913Z","shell.execute_reply.started":"2024-11-04T18:02:23.085156Z","shell.execute_reply":"2024-11-04T18:03:41.407012Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.5000\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.use_cache = False\nmodel.config.pretraining_tp = 1\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:41.409161Z","iopub.execute_input":"2024-11-04T18:03:41.409531Z","iopub.status.idle":"2024-11-04T18:03:41.427829Z","shell.execute_reply.started":"2024-11-04T18:03:41.409488Z","shell.execute_reply":"2024-11-04T18:03:41.427163Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# LoRA configuration\npeft_config = LoraConfig(\n    r= 32,          \n    lora_alpha= 32,\n    lora_dropout=0.07,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:41.428819Z","iopub.execute_input":"2024-11-04T18:03:41.429107Z","iopub.status.idle":"2024-11-04T18:03:41.433536Z","shell.execute_reply.started":"2024-11-04T18:03:41.429076Z","shell.execute_reply":"2024-11-04T18:03:41.432653Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Set training arguments\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    gradient_checkpointing=True,\n    gradient_accumulation_steps=1,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",       # Log at specific steps \n    learning_rate=1e-4,\n    group_by_length = True,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    warmup_steps = 5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    save_strategy=\"epoch\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:41.434677Z","iopub.execute_input":"2024-11-04T18:03:41.434964Z","iopub.status.idle":"2024-11-04T18:03:41.473554Z","shell.execute_reply.started":"2024-11-04T18:03:41.434934Z","shell.execute_reply":"2024-11-04T18:03:41.472657Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the function to generate prompt\ndef generate_prompt(example):\n#     print(example)\n    premise, hypothesis, label = example['premise'], example['hypothesis'], example['label']\n    prompt = (\n        f\"Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\n\"\n        f\"Premise: {premise}\\n\"\n        f\"Hypothesis: {hypothesis}\\n\"\n        f\"Choose the best answer:\\n\"\n        f\"If Hypothesis entails the Premise, Answer: 0\\n\"\n        f\"If Neutral, Answer: 1\\n\"\n        f\"If Hypothesis does not entail the Premise, Answer: 2\\n\"\n        f\"Answer only one numerical value. \\n\\n\"\n        f\"Answer: {label if label is not None else ''}\"\n    )\n    return prompt\n\ntrain_data = {\"prompt\": [generate_prompt(example) for example in dataset['train']], \"label\": [example[\"label\"] for example in dataset['train']]}\nval_data = {\"prompt\": [generate_prompt(example) for example in dataset['validation']], \"label\": [example[\"label\"] for example in dataset['validation']]}\ntest_data = {\"prompt\": [generate_prompt(example) for example in dataset['test']], \"label\": [example[\"label\"] for example in dataset['test']]}\n\ndataset_prompt = DatasetDict({\n    \"train\": Dataset.from_dict(train_data),\n    \"val\": Dataset.from_dict(val_data),\n    \"test\": Dataset.from_dict(test_data)\n})\n\n# Let's print a sample from each to verify the format\nprint(\"Sample from Train Dataset:\\n\", dataset_prompt['train'][0])\nprint(\"\\nSample from Validation Dataset:\\n\", dataset_prompt['val'][0])\nprint(\"\\nSample from Test Dataset:\\n\", dataset_prompt['test'][0])\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:41.474742Z","iopub.execute_input":"2024-11-04T18:03:41.475103Z","iopub.status.idle":"2024-11-04T18:03:41.615178Z","shell.execute_reply.started":"2024-11-04T18:03:41.475061Z","shell.execute_reply":"2024-11-04T18:03:41.614346Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Sample from Train Dataset:\n {'prompt': 'Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\nPremise: A person on a horse jumps over a broken down airplane.\\nHypothesis: A person is training his horse for a competition.\\nChoose the best answer:\\nIf Hypothesis entails the Premise, Answer: 0\\nIf Neutral, Answer: 1\\nIf Hypothesis does not entail the Premise, Answer: 2\\nAnswer only one numerical value. \\n\\nAnswer: 1', 'label': 1}\n\nSample from Validation Dataset:\n {'prompt': 'Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\nPremise: Two women are embracing while holding to go packages.\\nHypothesis: The sisters are hugging goodbye while holding to go packages after just eating lunch.\\nChoose the best answer:\\nIf Hypothesis entails the Premise, Answer: 0\\nIf Neutral, Answer: 1\\nIf Hypothesis does not entail the Premise, Answer: 2\\nAnswer only one numerical value. \\n\\nAnswer: 1', 'label': 1}\n\nSample from Test Dataset:\n {'prompt': 'Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\nPremise: This church choir sings to the masses as they sing joyous songs from the book at a church.\\nHypothesis: The church has cracks in the ceiling.\\nChoose the best answer:\\nIf Hypothesis entails the Premise, Answer: 0\\nIf Neutral, Answer: 1\\nIf Hypothesis does not entail the Premise, Answer: 2\\nAnswer only one numerical value. \\n\\nAnswer: 1', 'label': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_dataset(prompts, labels, tokenizer, max_length=128):\n    encodings = tokenizer(\n        prompts,\n        padding=\"max_length\",          # Pads all sequences to the max length specified\n        truncation=True,               # Truncates sequences longer than the max length\n        max_length=max_length,         # Sets the maximum sequence length\n        return_tensors=\"pt\"           # Returns PyTorch tensors\n    )\n    \n    # Add labels to the encodings\n    encodings['labels'] = torch.tensor(labels)  # Ensure labels are added as a tensor\n    return encodings","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:41.616259Z","iopub.execute_input":"2024-11-04T18:03:41.616567Z","iopub.status.idle":"2024-11-04T18:03:41.621510Z","shell.execute_reply.started":"2024-11-04T18:03:41.616533Z","shell.execute_reply":"2024-11-04T18:03:41.620700Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_prompts = [example['prompt'] for example in dataset_prompt['train']]\ntrain_labels = [example['label'] for example in dataset_prompt['train']]\nval_prompts = [example['prompt'] for example in dataset_prompt['val']]\nval_labels = [example['label'] for example in dataset_prompt['val']]\ntest_prompts = [example['prompt'] for example in dataset_prompt['test']]\ntest_labels = [example['label'] for example in dataset_prompt['test']]","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:41.622720Z","iopub.execute_input":"2024-11-04T18:03:41.623023Z","iopub.status.idle":"2024-11-04T18:03:41.718830Z","shell.execute_reply.started":"2024-11-04T18:03:41.622974Z","shell.execute_reply":"2024-11-04T18:03:41.717980Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_encodings = tokenize_dataset(train_prompts, train_labels, tokenizer)\nval_encodings = tokenize_dataset(val_prompts, val_labels, tokenizer)\ntest_encodings = tokenize_dataset(test_prompts, test_labels, tokenizer)\nprint(train_encodings)\n# Display a sample of the tokenized train dataset\nprint(\"Sample tokenized input IDs for Train Dataset:\\n\", train_encodings['input_ids'][0])\nprint(\"\\nSample attention mask for Train Dataset:\\n\", train_encodings['attention_mask'][0])\nprint(train_encodings['labels'][0])","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:41.719988Z","iopub.execute_input":"2024-11-04T18:03:41.720621Z","iopub.status.idle":"2024-11-04T18:03:42.001780Z","shell.execute_reply.started":"2024-11-04T18:03:41.720578Z","shell.execute_reply":"2024-11-04T18:03:42.000883Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n        [ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n        [ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n        ...,\n        [ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n        [ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n        [ 5569,   262,  1708,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 1, 0, 0, 0, 2, 2, 1, 0, 1, 1, 2, 0, 1, 2, 0, 0, 2, 0, 2, 2, 2, 0,\n        1, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0, 1, 0, 2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1,\n        1, 0, 2, 2, 2, 1, 0, 1, 2, 0, 2, 2, 0, 1, 1, 1, 1, 2, 0, 2, 2, 2, 0, 1,\n        1, 2, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 2, 1, 1, 1, 2, 0, 1, 0, 1, 2, 1, 0,\n        1, 1, 2, 1, 2, 2, 1, 0, 1, 2, 1, 0, 2, 2, 0, 2, 0, 1, 1, 0, 1, 1, 1, 2,\n        0, 0, 1, 2, 1, 2, 0, 1, 2, 2, 2, 2, 0, 1, 0, 1, 1, 0, 2, 2, 2, 0, 2, 2,\n        2, 2, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0, 2,\n        2, 1, 0, 2, 1, 2, 1, 0, 1, 0, 2, 1, 0, 2, 0, 2, 1, 1, 0, 0, 1, 2, 1, 2,\n        2, 1, 2, 2, 0, 2, 0, 1, 1, 0, 2, 0, 2, 0, 2, 1, 2, 1, 0, 1, 1, 1, 1, 0,\n        1, 2, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 1, 2, 1, 1,\n        0, 2, 0, 0, 1, 1, 0, 2, 2, 2, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 0, 2, 2, 1,\n        2, 0, 0, 2, 1, 1, 1, 0, 2, 1, 2, 1, 2, 2, 2, 2, 0, 0, 2, 1, 1, 0, 0, 2,\n        0, 2, 2, 1, 0, 1, 1, 0, 2, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 2, 0, 2, 2,\n        1, 2, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 2, 2,\n        2, 0, 2, 0, 1, 1, 2, 0, 1, 0, 2, 1, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1, 1, 0,\n        1, 2, 0, 0, 0, 0, 2, 1, 1, 1, 2, 2, 2, 0, 1, 1, 1, 2, 0, 2, 2, 2, 1, 2,\n        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 1, 2, 0, 0, 2, 0, 2, 2, 2, 1, 2,\n        0, 1, 1, 1, 2, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 2, 1,\n        0, 2, 1, 1, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 1, 2, 2, 2, 0, 0, 2,\n        1, 1, 0, 1, 1, 1, 2, 2, 1, 0, 1, 1, 0, 1, 2, 2, 0, 2, 1, 2, 0, 2, 0, 0,\n        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0,\n        1, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 1, 1, 2, 2, 0, 1, 0, 1, 0, 0, 2, 2, 2,\n        0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 0, 1, 0, 2, 1, 2, 1, 1, 2, 1, 0, 1, 2, 0,\n        2, 2, 2, 0, 1, 0, 1, 2, 2, 1, 0, 2, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 2, 0,\n        0, 1, 0, 1, 1, 2, 1, 2, 2, 0, 0, 2, 1, 2, 0, 0, 0, 2, 2, 2, 0, 1, 0, 1,\n        1, 1, 0, 0, 0, 2, 2, 0, 1, 1, 2, 0, 1, 2, 2, 2, 2, 0, 0, 0, 2, 0, 1, 0,\n        0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0,\n        1, 0, 2, 2, 0, 0, 1, 2, 0, 0, 1, 1, 2, 0, 2, 0, 1, 1, 1, 1, 0, 2, 0, 2,\n        0, 2, 2, 1, 1, 0, 1, 0, 0, 0, 2, 0, 2, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 0,\n        1, 2, 2, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 1, 2, 2, 2, 1, 2, 1, 0, 0, 1, 2,\n        0, 1, 1, 0, 0, 2, 0, 1, 2, 0, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0,\n        1, 2, 1, 1, 0, 0, 1, 2, 0, 0, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2,\n        2, 1, 1, 1, 0, 1, 2, 1, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 1, 0, 0, 1,\n        1, 1, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 2, 2, 2, 1, 0, 2, 1, 0, 2, 2,\n        2, 2, 0, 1, 1, 1, 1, 1, 0, 2, 0, 2, 2, 0, 2, 1, 2, 1, 1, 0, 2, 1, 0, 1,\n        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 0, 1, 2,\n        1, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 1,\n        0, 1, 2, 1, 2, 1, 2, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n        1, 2, 2, 0, 2, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 1, 1, 1, 1,\n        1, 0, 2, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 2, 2, 0, 2, 1, 0, 0, 2, 2, 0, 1,\n        1, 1, 1, 2, 0, 0, 1, 1, 2, 1, 2, 0, 0, 0, 2, 2, 1, 0, 1, 1, 0, 2, 2, 2,\n        2, 1, 1, 2, 2, 1, 0, 0, 2, 1, 0, 1, 0, 2, 1, 0])}\nSample tokenized input IDs for Train Dataset:\n tensor([ 5569,   262,  1708,  6929,   786,   290, 21209,   313,  8497,   290,\n         5004,  1771, 21209,   313,  8497, 35445,   262,  6929,   786,   393,\n          407,    13,   198, 24914,   786,    25,   317,  1048,   319,   257,\n         8223, 18045,   625,   257,  5445,   866, 19401,    13,   198, 49926,\n          313,  8497,    25,   317,  1048,   318,  3047,   465,  8223,   329,\n          257,  5449,    13,   198, 31851,   262,  1266,  3280,    25,   198,\n         1532, 21209,   313,  8497, 35445,   262,  6929,   786,    11, 23998,\n           25,   657,   198,  1532, 25627,    11, 23998,    25,   352,   198,\n         1532, 21209,   313,  8497,   857,   407, 39793,   262,  6929,   786,\n           11, 23998,    25,   362,   198, 33706,   691,   530, 29052,  1988,\n           13,   220,   198,   198, 33706,    25,   352, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])\n\nSample attention mask for Train Dataset:\n tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\ntensor(1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n# Create datasets\ntrain_dataset_custom = CustomDataset(train_encodings)\nval_dataset_custom = CustomDataset(val_encodings)\ntest_dataset_custom = CustomDataset(test_encodings)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:42.002852Z","iopub.execute_input":"2024-11-04T18:03:42.003173Z","iopub.status.idle":"2024-11-04T18:03:42.009410Z","shell.execute_reply.started":"2024-11-04T18:03:42.003140Z","shell.execute_reply":"2024-11-04T18:03:42.008457Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import transformers\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_custom,\n    eval_dataset=val_dataset_custom,\n    peft_config=peft_config,\n    max_seq_length= None,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:42.010492Z","iopub.execute_input":"2024-11-04T18:03:42.010758Z","iopub.status.idle":"2024-11-04T18:03:42.845227Z","shell.execute_reply.started":"2024-11-04T18:03:42.010730Z","shell.execute_reply":"2024-11-04T18:03:42.844320Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\n# Measure time taken for fine-tuning\nstart_time = time.time()\n\n# Fine-tune model\ntrainer.train()\n\n# Time taken for fine-tuning\ntime_taken = time.time() - start_time\nprint(f\"Time taken for fine-tuning: {time_taken:.2f} seconds\")\n\n\n# Save final model\ntrainer.save_model(\"./phi2_finetuned/final_model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:03:42.846472Z","iopub.execute_input":"2024-11-04T18:03:42.846785Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113364744444552, max=1.0‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab7a32709154132ad71be4bfc13c2c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241104_180352-dxowp1bv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lakshya1/huggingface/runs/dxowp1bv' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/lakshya1/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lakshya1/huggingface' target=\"_blank\">https://wandb.ai/lakshya1/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lakshya1/huggingface/runs/dxowp1bv' target=\"_blank\">https://wandb.ai/lakshya1/huggingface/runs/dxowp1bv</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='198' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [198/315 15:24 < 09:12, 0.21 it/s, Epoch 3.13/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.426900</td>\n      <td>0.610361</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.287900</td>\n      <td>0.601027</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.279400</td>\n      <td>0.599365</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get model parameter counts\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Trainable parameters: {trainable_params}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel\n\nf_model = AutoModelForCausalLM.from_pretrained('/kaggle/working/phi2_finetuned/final_model').to(\"cuda\")\n# f_model = PeftModel.from_pretrained(model, \"/kaggle/working/phi2_finetuned/final_model\",torch_dtype=torch.float16,is_trainable=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_prompt(premise, hypothesis):\n    return (\n        f\"Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\n\"\n        f\"Premise: {premise}\\n\"\n        f\"Hypothesis: {hypothesis}\\n\"\n        f\"Choose the best answer:\\n\"\n        f\"If Hypothesis entails the Premise, Answer: 0\\n\"\n        f\"If Neutral, Answer: 1\\n\"\n        f\"If Hypothesis does not entail the Premise, Answer: 2\\n\"\n        f\"Answer only one numerical value. \\n\\n\"\n        f\"Answer:\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Define mapping of labels\nlabel_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\nfinetune_outputs = []\n# Evaluate on the test set\ncorrect = 0\ntotal = 0\n\nfor example in dataset['test']:\n    # Create prompt from premise and hypothesis\n    prompt = generate_prompt(example['premise'], example['hypothesis'])\n    \n    # Generate model's response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    output = f_model.generate(**inputs, max_new_tokens=15)\n    response = tokenizer.decode(output[0], skip_special_tokens=True).strip().lower()\n#     print(f\"Model response: {response}\")\n\n    # Find the last occurrence of \"answer: 0\", \"answer: 1\", or \"answer: 2\" in reverse order\n    last_answer = None\n\n    for ans in [\"answer: 0\", \"answer: 1\", \"answer: 2\"]:\n        pos = response.rfind(ans)  # Search from the end of the text\n        if pos != -1:\n            # Keep track of the last found answer if it appears later in the text\n            if last_answer is None or pos > response.rfind(last_answer):\n                last_answer = ans\n\n    output = None\n    if(last_answer == \"answer: 0\"):\n        output = 0\n    elif(last_answer == \"answer: 1\"):\n        output = 1\n    elif(last_answer == \"answer: 2\"):\n        output = 2\n    \n#     print(output)\n#     print(example['label'])\n    if output == example['label']:\n        correct += 1\n    total += 1\n    finetune_outputs.append(output)\n# Calculate and print accuracy\naccuracy = correct / total\nprint(f\"Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['test'][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define mapping of labels\nlabel_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n).to(device)\n\n\nfor i in range(100):\n    true_label = dataset['test'][i]['label']\n    if(pretrain_outputs[i] != true_label and finetune_outputs[i] == true_label):\n        prompt = generate_prompt(dataset['test'][i]['premise'], dataset['test'][i]['hypothesis'])\n\n    # Pretrained model inference\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        output_pretrained = model.generate(**inputs, max_new_tokens=15)\n        response_pretrained = tokenizer.decode(output_pretrained[0], skip_special_tokens=True).strip().lower()\n\n        # Fine-tuned model inference\n        output_finetuned = f_model.generate(**inputs, max_new_tokens=15)\n        response_finetuned = tokenizer.decode(output_finetuned[0], skip_special_tokens=True).strip().lower()\n        print(f\"True Label: {true_label}\")\n        print(f\"Pretrained:\\n {response_pretrained}\")\n        print(f\"Finetuned:\\n {response_finetuned}\")\n        \n    if(pretrain_outputs[i] != true_label and finetune_outputs[i] != true_label):\n        prompt = generate_prompt(dataset['test'][i]['premise'], dataset['test'][i]['hypothesis'])\n\n    # Pretrained model inference\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        output_pretrained = model.generate(**inputs, max_new_tokens=15)\n        response_pretrained = tokenizer.decode(output_pretrained[0], skip_special_tokens=True).strip().lower()\n\n        # Fine-tuned model inference\n        output_finetuned = f_model.generate(**inputs, max_new_tokens=15)\n        response_finetuned = tokenizer.decode(output_finetuned[0], skip_special_tokens=True).strip().lower()\n        print(f\"True Label: {true_label}\")\n        print(f\"Pretrained:\\n {response_pretrained}\")\n        print(f\"Finetuned:\\n {response_finetuned}\")\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}