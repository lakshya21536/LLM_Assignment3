{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-03T16:53:44.140488Z","iopub.status.busy":"2024-11-03T16:53:44.140094Z","iopub.status.idle":"2024-11-03T16:53:45.361672Z","shell.execute_reply":"2024-11-03T16:53:45.360687Z","shell.execute_reply.started":"2024-11-03T16:53:44.140451Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:32:00.394493Z","iopub.status.busy":"2024-11-04T04:32:00.394133Z","iopub.status.idle":"2024-11-04T04:32:24.653761Z","shell.execute_reply":"2024-11-04T04:32:24.652677Z","shell.execute_reply.started":"2024-11-04T04:32:00.394443Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\n","Collecting pip\n","  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n","Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.0\n","    Uninstalling pip-24.0:\n","      Successfully uninstalled pip-24.0\n","Successfully installed pip-24.3.1\n"]}],"source":["!pip install --upgrade pip"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:32:28.489419Z","iopub.status.busy":"2024-11-04T04:32:28.489041Z","iopub.status.idle":"2024-11-04T04:32:48.825890Z","shell.execute_reply":"2024-11-04T04:32:48.824957Z","shell.execute_reply.started":"2024-11-04T04:32:28.489381Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting peft\n","  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n","Collecting trl\n","  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n","Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\n","Requirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\n","Collecting transformers (from peft)\n","  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.1)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\n","Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n","Downloading trl-0.12.0-py3-none-any.whl (310 kB)\n","Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes, transformers, trl, peft\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.45.1\n","    Uninstalling transformers-4.45.1:\n","      Successfully uninstalled transformers-4.45.1\n","Successfully installed bitsandbytes-0.44.1 peft-0.13.2 transformers-4.46.1 trl-0.12.0\n"]}],"source":["!pip install peft trl bitsandbytes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:32:53.807595Z","iopub.status.busy":"2024-11-04T04:32:53.806915Z","iopub.status.idle":"2024-11-04T04:33:12.048033Z","shell.execute_reply":"2024-11-04T04:33:12.047303Z","shell.execute_reply.started":"2024-11-04T04:32:53.807552Z"},"trusted":true},"outputs":[],"source":["# Install and import the necessary libraries\n","# !pip install torch peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 accelerate einops \n","\n","import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    pipeline,\n",")\n","from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:33:21.557697Z","iopub.status.busy":"2024-11-04T04:33:21.557316Z","iopub.status.idle":"2024-11-04T04:33:21.561873Z","shell.execute_reply":"2024-11-04T04:33:21.560966Z","shell.execute_reply.started":"2024-11-04T04:33:21.557661Z"},"trusted":true},"outputs":[],"source":["# Model\n","base_model = \"microsoft/phi-2\"\n","new_model = \"phi-2-finetuned\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:33:25.157210Z","iopub.status.busy":"2024-11-04T04:33:25.156830Z","iopub.status.idle":"2024-11-04T04:33:59.221100Z","shell.execute_reply":"2024-11-04T04:33:59.220361Z","shell.execute_reply.started":"2024-11-04T04:33:25.157175Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94b66491402144dc85a11eea51af4a7d","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/16.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8de78b962042467ea6b7c2154708f901","version_major":2,"version_minor":0},"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/412k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a4a33ab3b1244b295ee6ed469c2b729","version_major":2,"version_minor":0},"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/413k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e73fc1cf9540419e86ac2db5f19b002e","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/19.6M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b6dd5d662cc48bf8c2dde5092accd7b","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2437697b8fb343c6b72b9b51958cfdb0","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69298b196f06464f875d04ba20c00bb9","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/550152 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset, DatasetDict, Dataset\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","import torch\n","\n","def load_and_sample_snli():\n","    dataset = load_dataset(\"stanfordnlp/snli\")\n","    \n","    # Sampling every 550th sample for training, every 100th for test and validation\n","    train_samples = dataset['train'].select(range(0, len(dataset['train']), 550))[:1000]\n","    test_samples = dataset['test'].select(range(0, len(dataset['test']), 100))[:100]\n","    validation_samples = dataset['validation'].select(range(0, len(dataset['validation']), 100))[:100]\n","    \n","    return DatasetDict({\n","        'train': Dataset.from_dict(train_samples),\n","        'test': Dataset.from_dict(test_samples),\n","        'validation': Dataset.from_dict(validation_samples)\n","    })\n","\n","dataset = load_and_sample_snli()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:34:04.154129Z","iopub.status.busy":"2024-11-04T04:34:04.153734Z","iopub.status.idle":"2024-11-04T04:34:23.664713Z","shell.execute_reply":"2024-11-04T04:34:23.663966Z","shell.execute_reply.started":"2024-11-04T04:34:04.154091Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f840c605c244931857796932cb56ef3","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c6a2d6a10874b00970adb5a7773e042","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7dec521168b5461597d09324c010fdda","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"337ec8e6eca9410f88b0fe7882e76755","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9f19ddd9aed41cc8076a0fdff4f0a1b","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f26582336ed34547acff657f1503ef65","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n","tokenizer.pad_token = tokenizer.unk_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:35:17.474524Z","iopub.status.busy":"2024-11-04T04:35:17.473666Z","iopub.status.idle":"2024-11-04T04:37:48.918671Z","shell.execute_reply":"2024-11-04T04:37:48.917935Z","shell.execute_reply.started":"2024-11-04T04:35:17.474475Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3424db98032840a28bc5b1ff8e1ca519","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfa1b4fdd57440ad9c3db50bad5f9032","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cc0703089d44b89b3797d155731990c","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9da8062cc67a43b4a7fc2d343835da7b","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f7c4c8e75de4799807c294e3202f1fd","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0915ad7c750b46d4b30a7d5ed75c0cd7","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"526e07ef77c44bafaa91ef992ba055fb","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Quantization configuration\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quant=False,\n",")\n","\n","# Load base moodel\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=bnb_config,\n","    trust_remote_code=True,\n","    device_map={\"\": 0}\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:37:55.276079Z","iopub.status.busy":"2024-11-04T04:37:55.275315Z","iopub.status.idle":"2024-11-04T04:37:55.280920Z","shell.execute_reply":"2024-11-04T04:37:55.279860Z","shell.execute_reply.started":"2024-11-04T04:37:55.276032Z"},"trusted":true},"outputs":[],"source":["def generate_prompt(premise, hypothesis):\n","    return (\n","        f\"Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\n\"\n","        f\"Premise: {premise}\\n\"\n","        f\"Hypothesis: {hypothesis}\\n\"\n","        f\"Choose the best answer:\\n\"\n","        f\"If Hypothesis entails the Premise, Answer: 0\\n\"\n","        f\"If Neutral, Answer: 1\\n\"\n","        f\"If Hypothesis does not entail the Premise, Answer: 2\\n\"\n","        f\"Answer only one numerical value. \\n\\n\"\n","        f\"Answer:\"\n","    )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:37:58.348758Z","iopub.status.busy":"2024-11-04T04:37:58.348374Z","iopub.status.idle":"2024-11-04T04:39:14.025623Z","shell.execute_reply":"2024-11-04T04:39:14.024681Z","shell.execute_reply.started":"2024-11-04T04:37:58.348715Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.5000\n"]}],"source":["import re\n","\n","# Define mapping of labels\n","label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n","\n","# Evaluate on the test set\n","correct = 0\n","total = 0\n","\n","for example in dataset['test']:\n","    # Create prompt from premise and hypothesis\n","    prompt = generate_prompt(example['premise'], example['hypothesis'])\n","    \n","    # Generate model's response\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    output = model.generate(**inputs, max_new_tokens=15)\n","    response = tokenizer.decode(output[0], skip_special_tokens=True).strip().lower()\n","#     print(f\"Model response: {response}\")\n","\n","    # Find the last occurrence of \"answer: 0\", \"answer: 1\", or \"answer: 2\" in reverse order\n","    last_answer = None\n","\n","    for ans in [\"answer: 0\", \"answer: 1\", \"answer: 2\"]:\n","        pos = response.rfind(ans)  # Search from the end of the text\n","        if pos != -1:\n","            # Keep track of the last found answer if it appears later in the text\n","            if last_answer is None or pos > response.rfind(last_answer):\n","                last_answer = ans\n","\n","    output = None\n","    if(last_answer == \"answer: 0\"):\n","        output = 0\n","    elif(last_answer == \"answer: 1\"):\n","        output = 1\n","    elif(last_answer == \"answer: 2\"):\n","        output = 2\n","    \n","#     print(output)\n","#     print(example['label'])\n","    if output == example['label']:\n","        correct += 1\n","    total += 1\n","\n","# Calculate and print accuracy\n","accuracy = correct / total\n","print(f\"Accuracy: {accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:39:18.694873Z","iopub.status.busy":"2024-11-04T04:39:18.693950Z","iopub.status.idle":"2024-11-04T04:39:18.711897Z","shell.execute_reply":"2024-11-04T04:39:18.711177Z","shell.execute_reply.started":"2024-11-04T04:39:18.694833Z"},"trusted":true},"outputs":[],"source":["model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:39:32.219066Z","iopub.status.busy":"2024-11-04T04:39:32.218712Z","iopub.status.idle":"2024-11-04T04:39:32.223935Z","shell.execute_reply":"2024-11-04T04:39:32.222968Z","shell.execute_reply.started":"2024-11-04T04:39:32.219032Z"},"trusted":true},"outputs":[],"source":["# LoRA configuration\n","peft_config = LoraConfig(\n","    r= 32,          \n","    lora_alpha= 32,\n","    lora_dropout=0.07,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"] \n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:39:38.645920Z","iopub.status.busy":"2024-11-04T04:39:38.645542Z","iopub.status.idle":"2024-11-04T04:39:38.682249Z","shell.execute_reply":"2024-11-04T04:39:38.681329Z","shell.execute_reply.started":"2024-11-04T04:39:38.645882Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["# Set training arguments\n","training_arguments = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",       # Log at specific steps \n","    learning_rate=1e-4,\n","    optim=\"adamw_8bit\",\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    warmup_steps = 5,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    save_strategy=\"epoch\"\n",")\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:39:46.482470Z","iopub.status.busy":"2024-11-04T04:39:46.481747Z","iopub.status.idle":"2024-11-04T04:39:46.620558Z","shell.execute_reply":"2024-11-04T04:39:46.619633Z","shell.execute_reply.started":"2024-11-04T04:39:46.482428Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample from Train Dataset:\n"," {'prompt': 'Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\nPremise: A person on a horse jumps over a broken down airplane.\\nHypothesis: A person is training his horse for a competition.\\nChoose the best answer:\\nIf Hypothesis entails the Premise, Answer: 0\\nIf Neutral, Answer: 1\\nIf Hypothesis does not entail the Premise, Answer: 2\\nAnswer only one numerical value. \\n\\nAnswer: 1', 'label': 1}\n","\n","Sample from Validation Dataset:\n"," {'prompt': 'Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\nPremise: Two women are embracing while holding to go packages.\\nHypothesis: The sisters are hugging goodbye while holding to go packages after just eating lunch.\\nChoose the best answer:\\nIf Hypothesis entails the Premise, Answer: 0\\nIf Neutral, Answer: 1\\nIf Hypothesis does not entail the Premise, Answer: 2\\nAnswer only one numerical value. \\n\\nAnswer: 1', 'label': 1}\n","\n","Sample from Test Dataset:\n"," {'prompt': 'Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\nPremise: This church choir sings to the masses as they sing joyous songs from the book at a church.\\nHypothesis: The church has cracks in the ceiling.\\nChoose the best answer:\\nIf Hypothesis entails the Premise, Answer: 0\\nIf Neutral, Answer: 1\\nIf Hypothesis does not entail the Premise, Answer: 2\\nAnswer only one numerical value. \\n\\nAnswer: 1', 'label': 1}\n"]}],"source":["# Define the function to generate prompt\n","def generate_prompt(example):\n","#     print(example)\n","    premise, hypothesis, label = example['premise'], example['hypothesis'], example['label']\n","    prompt = (\n","        f\"Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\n\"\n","        f\"Premise: {premise}\\n\"\n","        f\"Hypothesis: {hypothesis}\\n\"\n","        f\"Choose the best answer:\\n\"\n","        f\"If Hypothesis entails the Premise, Answer: 0\\n\"\n","        f\"If Neutral, Answer: 1\\n\"\n","        f\"If Hypothesis does not entail the Premise, Answer: 2\\n\"\n","        f\"Answer only one numerical value. \\n\\n\"\n","        f\"Answer: {label if label is not None else ''}\"\n","    )\n","    return prompt\n","\n","train_data = {\"prompt\": [generate_prompt(example) for example in dataset['train']], \"label\": [example[\"label\"] for example in dataset['train']]}\n","val_data = {\"prompt\": [generate_prompt(example) for example in dataset['validation']], \"label\": [example[\"label\"] for example in dataset['validation']]}\n","test_data = {\"prompt\": [generate_prompt(example) for example in dataset['test']], \"label\": [example[\"label\"] for example in dataset['test']]}\n","\n","dataset_prompt = DatasetDict({\n","    \"train\": Dataset.from_dict(train_data),\n","    \"val\": Dataset.from_dict(val_data),\n","    \"test\": Dataset.from_dict(test_data)\n","})\n","\n","# Let's print a sample from each to verify the format\n","print(\"Sample from Train Dataset:\\n\", dataset_prompt['train'][0])\n","print(\"\\nSample from Validation Dataset:\\n\", dataset_prompt['val'][0])\n","print(\"\\nSample from Test Dataset:\\n\", dataset_prompt['test'][0])\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:39:54.874979Z","iopub.status.busy":"2024-11-04T04:39:54.874140Z","iopub.status.idle":"2024-11-04T04:39:54.880106Z","shell.execute_reply":"2024-11-04T04:39:54.879159Z","shell.execute_reply.started":"2024-11-04T04:39:54.874937Z"},"trusted":true},"outputs":[],"source":["def tokenize_dataset(prompts, labels, tokenizer, max_length=128):\n","    encodings = tokenizer(\n","        prompts,\n","        padding=\"max_length\",          # Pads all sequences to the max length specified\n","        truncation=True,               # Truncates sequences longer than the max length\n","        max_length=max_length,         # Sets the maximum sequence length\n","        return_tensors=\"pt\"           # Returns PyTorch tensors\n","    )\n","    \n","    # Add labels to the encodings\n","    encodings['labels'] = torch.tensor(labels)  # Ensure labels are added as a tensor\n","    return encodings"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:39:58.669771Z","iopub.status.busy":"2024-11-04T04:39:58.668860Z","iopub.status.idle":"2024-11-04T04:39:58.764798Z","shell.execute_reply":"2024-11-04T04:39:58.763855Z","shell.execute_reply.started":"2024-11-04T04:39:58.669711Z"},"trusted":true},"outputs":[],"source":["train_prompts = [example['prompt'] for example in dataset_prompt['train']]\n","train_labels = [example['label'] for example in dataset_prompt['train']]\n","val_prompts = [example['prompt'] for example in dataset_prompt['val']]\n","val_labels = [example['label'] for example in dataset_prompt['val']]\n","test_prompts = [example['prompt'] for example in dataset_prompt['test']]\n","test_labels = [example['label'] for example in dataset_prompt['test']]"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:40:01.843365Z","iopub.status.busy":"2024-11-04T04:40:01.842944Z","iopub.status.idle":"2024-11-04T04:40:02.102469Z","shell.execute_reply":"2024-11-04T04:40:02.101507Z","shell.execute_reply.started":"2024-11-04T04:40:01.843327Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n","        [ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n","        [ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n","        ...,\n","        [ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n","        [ 5569,   262,  1708,  ..., 50256, 50256, 50256],\n","        [ 5569,   262,  1708,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 1, 0, 0, 0, 2, 2, 1, 0, 1, 1, 2, 0, 1, 2, 0, 0, 2, 0, 2, 2, 2, 0,\n","        1, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0, 1, 0, 2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1,\n","        1, 0, 2, 2, 2, 1, 0, 1, 2, 0, 2, 2, 0, 1, 1, 1, 1, 2, 0, 2, 2, 2, 0, 1,\n","        1, 2, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 2, 1, 1, 1, 2, 0, 1, 0, 1, 2, 1, 0,\n","        1, 1, 2, 1, 2, 2, 1, 0, 1, 2, 1, 0, 2, 2, 0, 2, 0, 1, 1, 0, 1, 1, 1, 2,\n","        0, 0, 1, 2, 1, 2, 0, 1, 2, 2, 2, 2, 0, 1, 0, 1, 1, 0, 2, 2, 2, 0, 2, 2,\n","        2, 2, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0, 2,\n","        2, 1, 0, 2, 1, 2, 1, 0, 1, 0, 2, 1, 0, 2, 0, 2, 1, 1, 0, 0, 1, 2, 1, 2,\n","        2, 1, 2, 2, 0, 2, 0, 1, 1, 0, 2, 0, 2, 0, 2, 1, 2, 1, 0, 1, 1, 1, 1, 0,\n","        1, 2, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 1, 2, 1, 1,\n","        0, 2, 0, 0, 1, 1, 0, 2, 2, 2, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 0, 2, 2, 1,\n","        2, 0, 0, 2, 1, 1, 1, 0, 2, 1, 2, 1, 2, 2, 2, 2, 0, 0, 2, 1, 1, 0, 0, 2,\n","        0, 2, 2, 1, 0, 1, 1, 0, 2, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 2, 0, 2, 2,\n","        1, 2, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 2, 2,\n","        2, 0, 2, 0, 1, 1, 2, 0, 1, 0, 2, 1, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1, 1, 0,\n","        1, 2, 0, 0, 0, 0, 2, 1, 1, 1, 2, 2, 2, 0, 1, 1, 1, 2, 0, 2, 2, 2, 1, 2,\n","        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 1, 2, 0, 0, 2, 0, 2, 2, 2, 1, 2,\n","        0, 1, 1, 1, 2, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 2, 1,\n","        0, 2, 1, 1, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 1, 2, 2, 2, 0, 0, 2,\n","        1, 1, 0, 1, 1, 1, 2, 2, 1, 0, 1, 1, 0, 1, 2, 2, 0, 2, 1, 2, 0, 2, 0, 0,\n","        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0,\n","        1, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 1, 1, 2, 2, 0, 1, 0, 1, 0, 0, 2, 2, 2,\n","        0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 0, 1, 0, 2, 1, 2, 1, 1, 2, 1, 0, 1, 2, 0,\n","        2, 2, 2, 0, 1, 0, 1, 2, 2, 1, 0, 2, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 2, 0,\n","        0, 1, 0, 1, 1, 2, 1, 2, 2, 0, 0, 2, 1, 2, 0, 0, 0, 2, 2, 2, 0, 1, 0, 1,\n","        1, 1, 0, 0, 0, 2, 2, 0, 1, 1, 2, 0, 1, 2, 2, 2, 2, 0, 0, 0, 2, 0, 1, 0,\n","        0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0,\n","        1, 0, 2, 2, 0, 0, 1, 2, 0, 0, 1, 1, 2, 0, 2, 0, 1, 1, 1, 1, 0, 2, 0, 2,\n","        0, 2, 2, 1, 1, 0, 1, 0, 0, 0, 2, 0, 2, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 0,\n","        1, 2, 2, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 1, 2, 2, 2, 1, 2, 1, 0, 0, 1, 2,\n","        0, 1, 1, 0, 0, 2, 0, 1, 2, 0, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0,\n","        1, 2, 1, 1, 0, 0, 1, 2, 0, 0, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2,\n","        2, 1, 1, 1, 0, 1, 2, 1, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 1, 0, 0, 1,\n","        1, 1, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 2, 2, 2, 1, 0, 2, 1, 0, 2, 2,\n","        2, 2, 0, 1, 1, 1, 1, 1, 0, 2, 0, 2, 2, 0, 2, 1, 2, 1, 1, 0, 2, 1, 0, 1,\n","        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 0, 1, 2,\n","        1, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 1,\n","        0, 1, 2, 1, 2, 1, 2, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n","        1, 2, 2, 0, 2, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 1, 1, 1, 1,\n","        1, 0, 2, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 2, 2, 0, 2, 1, 0, 0, 2, 2, 0, 1,\n","        1, 1, 1, 2, 0, 0, 1, 1, 2, 1, 2, 0, 0, 0, 2, 2, 1, 0, 1, 1, 0, 2, 2, 2,\n","        2, 1, 1, 2, 2, 1, 0, 0, 2, 1, 0, 1, 0, 2, 1, 0])}\n","Sample tokenized input IDs for Train Dataset:\n"," tensor([ 5569,   262,  1708,  6929,   786,   290, 21209,   313,  8497,   290,\n","         5004,  1771, 21209,   313,  8497, 35445,   262,  6929,   786,   393,\n","          407,    13,   198, 24914,   786,    25,   317,  1048,   319,   257,\n","         8223, 18045,   625,   257,  5445,   866, 19401,    13,   198, 49926,\n","          313,  8497,    25,   317,  1048,   318,  3047,   465,  8223,   329,\n","          257,  5449,    13,   198, 31851,   262,  1266,  3280,    25,   198,\n","         1532, 21209,   313,  8497, 35445,   262,  6929,   786,    11, 23998,\n","           25,   657,   198,  1532, 25627,    11, 23998,    25,   352,   198,\n","         1532, 21209,   313,  8497,   857,   407, 39793,   262,  6929,   786,\n","           11, 23998,    25,   362,   198, 33706,   691,   530, 29052,  1988,\n","           13,   220,   198,   198, 33706,    25,   352, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])\n","\n","Sample attention mask for Train Dataset:\n"," tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0])\n","tensor(1)\n"]}],"source":["train_encodings = tokenize_dataset(train_prompts, train_labels, tokenizer)\n","val_encodings = tokenize_dataset(val_prompts, val_labels, tokenizer)\n","test_encodings = tokenize_dataset(test_prompts, test_labels, tokenizer)\n","print(train_encodings)\n","# Display a sample of the tokenized train dataset\n","print(\"Sample tokenized input IDs for Train Dataset:\\n\", train_encodings['input_ids'][0])\n","print(\"\\nSample attention mask for Train Dataset:\\n\", train_encodings['attention_mask'][0])\n","print(train_encodings['labels'][0])"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:40:10.520101Z","iopub.status.busy":"2024-11-04T04:40:10.519714Z","iopub.status.idle":"2024-11-04T04:40:10.526708Z","shell.execute_reply":"2024-11-04T04:40:10.525675Z","shell.execute_reply.started":"2024-11-04T04:40:10.520062Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings['input_ids'])\n","\n","# Create datasets\n","train_dataset_custom = CustomDataset(train_encodings)\n","val_dataset_custom = CustomDataset(val_encodings)\n","test_dataset_custom = CustomDataset(test_encodings)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:40:26.420026Z","iopub.status.busy":"2024-11-04T04:40:26.419655Z","iopub.status.idle":"2024-11-04T04:40:27.092437Z","shell.execute_reply":"2024-11-04T04:40:27.091486Z","shell.execute_reply.started":"2024-11-04T04:40:26.419977Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]}],"source":["import transformers\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_dataset_custom,\n","    eval_dataset=val_dataset_custom,\n","    peft_config=peft_config,\n","    max_seq_length= None,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n",")\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T04:40:31.156753Z","iopub.status.busy":"2024-11-04T04:40:31.155881Z","iopub.status.idle":"2024-11-04T05:02:44.578147Z","shell.execute_reply":"2024-11-04T05:02:44.577340Z","shell.execute_reply.started":"2024-11-04T04:40:31.156708Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4c02df3493847db8eb89ec8bf219527","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113598844445328, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241104_044042-buyqk7rv</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/lakshya1/huggingface/runs/buyqk7rv' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/lakshya1/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/lakshya1/huggingface' target=\"_blank\">https://wandb.ai/lakshya1/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/lakshya1/huggingface/runs/buyqk7rv' target=\"_blank\">https://wandb.ai/lakshya1/huggingface/runs/buyqk7rv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [315/315 21:49, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.482900</td>\n","      <td>0.617929</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.295600</td>\n","      <td>0.607264</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.289100</td>\n","      <td>0.604472</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.286000</td>\n","      <td>0.602296</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.283600</td>\n","      <td>0.602463</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"name":"stdout","output_type":"stream","text":["Time taken for fine-tuning: 1327.96 seconds\n"]}],"source":["import time\n","\n","# Measure time taken for fine-tuning\n","start_time = time.time()\n","\n","# Fine-tune model\n","trainer.train()\n","\n","# Time taken for fine-tuning\n","time_taken = time.time() - start_time\n","print(f\"Time taken for fine-tuning: {time_taken:.2f} seconds\")\n","\n","\n","# Save final model\n","model.save_pretrained(\"./phi2_finetuned/final_model\")\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T05:02:46.441619Z","iopub.status.busy":"2024-11-04T05:02:46.441230Z","iopub.status.idle":"2024-11-04T05:02:46.461924Z","shell.execute_reply":"2024-11-04T05:02:46.460968Z","shell.execute_reply.started":"2024-11-04T05:02:46.441582Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 1542364160\n","Trainable parameters: 20971520\n"]}],"source":["# Get model parameter counts\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Total parameters: {total_params}\")\n","print(f\"Trainable parameters: {trainable_params}\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T05:02:56.081588Z","iopub.status.busy":"2024-11-04T05:02:56.080631Z","iopub.status.idle":"2024-11-04T05:02:56.593038Z","shell.execute_reply":"2024-11-04T05:02:56.592047Z","shell.execute_reply.started":"2024-11-04T05:02:56.081533Z"},"trusted":true},"outputs":[],"source":["from peft import PeftModel\n","\n","f_model = AutoModelForCausalLM.from_pretrained('/kaggle/working/phi2_finetuned/final_model').to(\"cuda\")\n","# f_model = PeftModel.from_pretrained(model, \"/kaggle/working/phi2_finetuned/final_model\",torch_dtype=torch.float16,is_trainable=False)\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T05:04:21.345558Z","iopub.status.busy":"2024-11-04T05:04:21.345158Z","iopub.status.idle":"2024-11-04T05:04:21.352172Z","shell.execute_reply":"2024-11-04T05:04:21.351245Z","shell.execute_reply.started":"2024-11-04T05:04:21.345521Z"},"trusted":true},"outputs":[],"source":["def generate_prompt(premise, hypothesis):\n","    return (\n","        f\"Read the following Premise and Hypothesis and determine whether Hypothesis entails the Premise or not.\\n\"\n","        f\"Premise: {premise}\\n\"\n","        f\"Hypothesis: {hypothesis}\\n\"\n","        f\"Choose the best answer:\\n\"\n","        f\"If Hypothesis entails the Premise, Answer: 0\\n\"\n","        f\"If Neutral, Answer: 1\\n\"\n","        f\"If Hypothesis does not entail the Premise, Answer: 2\\n\"\n","        f\"Answer only one numerical value. \\n\\n\"\n","        f\"Answer:\"\n","    )"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T05:04:24.965643Z","iopub.status.busy":"2024-11-04T05:04:24.965127Z","iopub.status.idle":"2024-11-04T05:06:47.132605Z","shell.execute_reply":"2024-11-04T05:06:47.131670Z","shell.execute_reply.started":"2024-11-04T05:04:24.965603Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.8700\n"]}],"source":["import re\n","\n","# Define mapping of labels\n","label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n","finetune_outputs = []\n","# Evaluate on the test set\n","correct = 0\n","total = 0\n","\n","for example in dataset['test']:\n","    # Create prompt from premise and hypothesis\n","    prompt = generate_prompt(example['premise'], example['hypothesis'])\n","    \n","    # Generate model's response\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    output = f_model.generate(**inputs, max_new_tokens=15)\n","    response = tokenizer.decode(output[0], skip_special_tokens=True).strip().lower()\n","#     print(f\"Model response: {response}\")\n","\n","    # Find the last occurrence of \"answer: 0\", \"answer: 1\", or \"answer: 2\" in reverse order\n","    last_answer = None\n","\n","    for ans in [\"answer: 0\", \"answer: 1\", \"answer: 2\"]:\n","        pos = response.rfind(ans)  # Search from the end of the text\n","        if pos != -1:\n","            # Keep track of the last found answer if it appears later in the text\n","            if last_answer is None or pos > response.rfind(last_answer):\n","                last_answer = ans\n","\n","    output = None\n","    if(last_answer == \"answer: 0\"):\n","        output = 0\n","    elif(last_answer == \"answer: 1\"):\n","        output = 1\n","    elif(last_answer == \"answer: 2\"):\n","        output = 2\n","    \n","#     print(output)\n","#     print(example['label'])\n","    if output == example['label']:\n","        correct += 1\n","    total += 1\n","    finetune_outputs.append(output)\n","# Calculate and print accuracy\n","accuracy = correct / total\n","print(f\"Accuracy: {accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define mapping of labels\n","label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=bnb_config,\n","    trust_remote_code=True,\n",").to(device)\n","\n","\n","for i in range(100):\n","    true_label = dataset['test'][i]['label']\n","    if(pretrain_outputs[i] != true_label and finetune_outputs[i] == true_label):\n","        prompt = generate_prompt(dataset['test'][i]['premise'], dataset['test'][i]['hypothesis'])\n","\n","    # Pretrained model inference\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","        output_pretrained = model.generate(**inputs, max_new_tokens=15)\n","        response_pretrained = tokenizer.decode(output_pretrained[0], skip_special_tokens=True).strip().lower()\n","\n","        # Fine-tuned model inference\n","        output_finetuned = f_model.generate(**inputs, max_new_tokens=15)\n","        response_finetuned = tokenizer.decode(output_finetuned[0], skip_special_tokens=True).strip().lower()\n","        print(f\"True Label: {true_label}\")\n","        print(f\"Pretrained:\\n {response_pretrained}\")\n","        print(f\"Finetuned:\\n {response_finetuned}\")\n","        \n","    if(pretrain_outputs[i] != true_label and finetune_outputs[i] != true_label):\n","        prompt = generate_prompt(dataset['test'][i]['premise'], dataset['test'][i]['hypothesis'])\n","\n","    # Pretrained model inference\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","        output_pretrained = model.generate(**inputs, max_new_tokens=15)\n","        response_pretrained = tokenizer.decode(output_pretrained[0], skip_special_tokens=True).strip().lower()\n","\n","        # Fine-tuned model inference\n","        output_finetuned = f_model.generate(**inputs, max_new_tokens=15)\n","        response_finetuned = tokenizer.decode(output_finetuned[0], skip_special_tokens=True).strip().lower()\n","        print(f\"True Label: {true_label}\")\n","        print(f\"Pretrained:\\n {response_pretrained}\")\n","        print(f\"Finetuned:\\n {response_finetuned}\")\n","    \n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
